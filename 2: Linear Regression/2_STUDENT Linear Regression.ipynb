{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIT 2: LINEAR REGRESSION\n",
    "___\n",
    "\n",
    "This unit has a lot of vocabulary.  I'm sorry.  But these terms are important, and we'll use them throughout the course.  So, try to stay focused and push through all these terms.\n",
    "\n",
    "# INPUT AND OUTPUT\n",
    "\n",
    "In any set of data that we'll analyze there will be input data and output data.  The assumption is that the input data somehow combines to produce the output data; that they are **correlated**.  Sometimes, however, this just isn't the case.  Sometimes the input data is not connected to the output data, so we always have to consider this as a possibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/graph1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we want to understand how the number of hours studied (input) can predict the score on a test (output).  Here, it is obvious that there is a *correlation* between hours studied and score.  Later we'll go into detail about how to handle multiple input criteria, but in this unit we're going to just focus on one input variable.  This is called **linear regression**.\n",
    "\n",
    "In many of the datasets that we study in this course, there are multiple input criteria contributing to one output.  In machine learning, we call input variables **features** and the output data is called a **label**.\n",
    "\n",
    "\n",
    "***Input data = features<br>\n",
    "Output data = label***\n",
    "\n",
    "# COEFFICIENTS\n",
    "\n",
    "Clearly not all features contribute the same amount to the label.  In the above example, study hours was clearly more important than daily temperature.  So, how to make one feature count more?  We introduce a weighting system... *coefficients*.\n",
    "\n",
    "In the above example, our first guess at a model might look something like this...\n",
    "\n",
    "```\n",
    "score = C1 • study hours\t\t where C1 is a coefficient\n",
    "```\n",
    "\n",
    "This looks suspiciously like the equation for a line...\n",
    "\n",
    "```\n",
    "y = m x + b\n",
    "```\n",
    "\n",
    "The one thing we're missing is the y-intercept.  And if you look at the graph above, you can see that there is definitely a y-intercept around 20.  So, our second guess at a model looks like...\n",
    "\n",
    "\tscore = C1 • study hours + C0\t\twhere C0 is the y-intercept\n",
    "\n",
    "Great!  But for reasons that will be explained later, we're going to write this equation like this...\n",
    "\n",
    "\tscore = C0 + C1 • study hours\n",
    "\n",
    "\n",
    "Nice.\n",
    "\n",
    "So, when we plug the data into the linear regression machine learning software, it should produce a model that looks something like that above equation. Since we already know the features and the label, *the only thing we're interested in is the coefficients.* These are the weights of each feature.\n",
    "\n",
    "OK, I exaggerated when I said that the only thing we're interested in is the coefficients. We also want to know *how accurate this model is*.  Remember, there's a chance that there really is no genuine relationship between input and output, and no matter what the computer will try to make sense of it. So, we have to decide if this model makes sense.\n",
    "\n",
    "# MEASURING ERROR\n",
    "\n",
    "<img src=\"img/graph2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We're not going to go into the mathematical details of how to measure error; instead, we're going to get an intuitive sense of what error in the model means.\n",
    "\n",
    "If this line is a perfect fit for our model, then every data point would be exactly on the line. *This never happens.*  Instead, the data points are scattered around the model's prediction. The further a data point is from the line, the bigger the error.  The distance from a data point to the model's line is how we measure the error.  We add up all of these distances and that is the total error for this particular prediction. *For machine learning, we call the total error the **cost**;* I think the sense is that *the higher the cost, the worse the model's prediction is.*\n",
    "\n",
    "Conceptually, this is how error is measured for every model using any technique.  The computer tries its best to minimize the error distances (the cost) by changing the line.\n",
    "\n",
    "# GRADIENT DESCENT\n",
    "\n",
    "How exactly does the computer end up finding a solution to our model?  Again, we're skipping the math and instead focusing on an intuitive sense of things.\n",
    "\n",
    "<img src=\"img/graph3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "This image shows the cost for different values of the coefficients.  Remember those?  They are how we assign weights to the input variables. The height of the graph is the cost; red is high cost, purple is low cost.  As we vary the values of the coefficients, the cost changes. When we have a lot of features, we can use this gradient descent technique to try and find a solution.\n",
    "\n",
    "Pretend that we randomly pick some coefficients and end up at the x near the left red peak. (In reality, this is exactly what the computer does... it just picks some random values to start with.) We get the cost with these coefficients, and then try again by changing the value of the coefficients. If the cost goes up, we change the coefficients in the other direction.  Gradually, the computer steps down from the red peak, into the yellow range, the green range, and finally ends up in the low purple area. This is the lowest cost we can get, and thus the best fit.\n",
    "\n",
    "Now, looking at this image we can see that there is another low blue area. What if the gradient descent ended up there instead? This happens quite often. That's called a local minimum, whereas we're looking for a global minimum.  How do we prevent this?  We go through the entire process multiple times and hope that one of the paths down the mountain leads to the global minimum, but sometimes it is impossible to guarantee that we're at the global minimum. Such is life.\n",
    "\n",
    "> **(read this if you're more interested!)** A really cool way to visualize this process is if you were to put a ball at some point on the 'terrain' of the graph, it would roll downhill, thus minimizing the cost function. I'm guessing you can visualize that happening and having the ball settle at the lowest point, even if it is a local minimum. But chew on this: what if the ball has momentum? Some machine learning algorithms add a sort of 'momentum' to their gradient descent to address the issue of local minima. The ball rolls downhill, but has some momentum so that it can get up a smaller hill to eventually settle on a lower minimum. Cool, right?\n",
    "\n",
    "# PLOTTING DATA\n",
    "\n",
    "Before we actually create a model, let's plot some data.  Like we talked about at the beginning of the course, humans are very good at seeing patterns in visual data.  So, seeing the data on a graph will help us give a general sense of what is going on.\n",
    "\n",
    "There is a plotting library for python called `matplotlib` that can do some truly amazing things.  It can create virtually any type of graph and allows you to customize nearly every aspect of the graph.  Here, we'll only introduce a simple scatter plot and a few of the more common options.\n",
    "\n",
    "The first dataset that we'll use for this lesson is called `cricket_chirps.csv`.  It is a ridiculously simple dataset, but we have to start somewhere, right?\n",
    "\n",
    "If you look at the dataset, you'll see that it has data for how temperature affects the number of cricket chirps per minute.  Notice that it has a header row. Go ahead and import the data using numpy (`cricket_chirps.csv` is in the data folder in the unit 2 folder).  Remember how to do this?  Since this is your first time doing it, I'll help you out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = np.loadtxt('data/cricket_chirps.csv', delimiter=',', skiprows=1, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice that the first column is `temperature` and the second column is `chirps / minute`.  So, think about this... which column is our feature (input) and which is our label (output)?\n",
    "\n",
    "Using your slicing skills, create two variables (`features` and `label`) and put the correct column of data in each variable.  Here is one weird quirk... later when we create our model, the library will expect the label list to be a simple array and the features list to be a multidimensional array.  Even though we only have 1 feature right now, we still need to create a list of lists.  Why does the library expect this?  Because later we'll have many features and, in fact, having only 1 feature is the exception to the rule.  Having only 1 feature is so simple that the library assumes that you would create a model in a simpler way.  But... we have to start at the beginning, so here we are.  Again, I'll help you out here, because this is your first time...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell\n",
    "\n",
    "label = data[ : , 1]        # chirps / minute\n",
    "features = data[ : , 0:1]   # temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "#this is something called a Jupyter magic command, it helps make matplotlib work better in the notebook environment\n",
    "#if you get an error that says 'no module ipympl found,' go into terminal and do pip install ipympl\n",
    "\n",
    "plt.plot(features, label, 'ro') \n",
    "\n",
    "plt.show() #shows the plot. this is interactive in the notebook, so you can use the tools on the bottom to move around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!  Try it out and see how it works.  (Also, we usually put all our import statements together at the top of the file.  You can rearrange the lines of code now to make the code look prettier.)\n",
    "\n",
    "First off, let's look at the `plot()` function. It takes 3 arguments in this order:\n",
    "\n",
    "\tplot( x-values, y-values, color and line type)\n",
    "\n",
    "So, notice that I put features in the `x-values` and label in the `y-values`.  Ensure that this makes sense to you.  In fact, whenever we're plotting data the output (the label) will be the y-values and the features (input) as the x-values. *The pattern is that x-values determine the y-values; input determines the output*\n",
    "\n",
    "Finally, I put something weird in for the color and line type... `'ro'`.  The `'r'` means red and the `'o'` means circles for the data points.  There are a TON of different ways to format your data points.  Go to this site to see all the options and fool around with some if you'd like...\n",
    "\n",
    "https://www.mathworks.com/help/matlab/ref/linespec.html\n",
    "\n",
    "Now, let's make this graph more presentable by adding axis labels, a chart title, and even some lines.  Before the `plt.plot(...)` line, add in the following lines of code...\n",
    "\n",
    "    plt.figure(num='Our First Model')\t\t# this adds a title to the plot window\n",
    "    plt.title('Temperature vs. Cricket Chirps')\t# this adds a title to the graph\n",
    "    plt.ylabel('chirps / minute')\t\t\t# this adds a label to the y-axis\n",
    "    plt.xlabel('temperature (F)')\t\t\t# this adds a label to the x-axis\n",
    "    plt.grid(True)\t\t\n",
    "    \n",
    "When you're done, it should like something like this...\n",
    "\n",
    "<img src=\"img/graph4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR FIRST MODEL\n",
    "\n",
    "Now that we've learned a bunch of information, let's try and come up with our first model.\n",
    "\n",
    "The package that we're going to be using a lot in this course is `sci-kit learn` (or `sklearn`), because it includes all of the different machine learning methods, is easy to use, contains a number of sample datasets, and has great documentation.  Sometimes it seems almost too easy when using sklearn.\n",
    "\n",
    "So, for this first unit (on linear regression) we're going to use the linear regression tool in `sklearn`.  Shocking, I know.  There are many options and parameters that you can set when using sklearn's linear regression tool, but we're going to run it with the default settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell\n",
    "\n",
    "from sklearn.linear_model import LinearRegression #imports sklearn\n",
    "\n",
    "lin = LinearRegression().fit(features, label) #regress linearly? is that english?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it.  After these 2 simple lines, the variable `lin` contains our linear regression model for the cricket chirp data.  Notice that the `fit()` function takes 2 variables in this order... the features multidimensional list and the label list.\n",
    "\n",
    "Now, let's get some details about our new model.  First, let's see how accurate it is.  Remember how error is measured by adding up all the distances from the data points to the model's prediction?  In this case, a linear regression score of 1 means that the all the data points are on the line of the model's prediction. The closer to 1 the score is, the better the model's accuracy (like percent accuracy in decimal form). To find the score of our model, add in this line of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'the score for our linear model is {lin.score(features, label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the actual heart of our model, the values for the coefficients (the weights of the input variables).  Remember how this is a linear regression, so the coefficients are the slope and the y-intercept.  Add in these lines to print these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = lin.coef_\n",
    "inter = lin.intercept_\n",
    "\n",
    "print (f'the slope is {slope}, and the y-intercept is {inter}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note here.\n",
    "\n",
    ">1) `intercept_` and `coef_` are special variables that are created whenever you run the `fit()` function.  They correspond to the y-intercept and the coefficients respectively.\n",
    "\n",
    ">2) The `coef_` variable returns a list, because the software expects there to usually be more than 1 feature.  This is the same reason why the features input to the `fit()` function expects a multidimensional list.  It is assumed that there are many features.  We just happen to be using a single feature for our first model.\n",
    "\n",
    ">3) The variable names end in the underscore character.  Don't forget this.  To type it, you press shift and the '-' key to the right of the number 0 on your keyboard at the same time.\n",
    "\n",
    ">4) We can also refer to `intercept_` as C0.  Refer back to the coefficients section of this unit to see why.\n",
    "\n",
    ">5) The solution to our model is a line since it's linear regression.  The equation of this solution is:\n",
    "\n",
    "        y = m x + b\n",
    "        output-data = coef_ • input-data + intercept_\n",
    "\n",
    "Finally, let's try out our model and see how it works.  Let's say we want to see how many chirps / minute happen when the temperature is 100 degrees.  Here's how we could find the answer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'if the temp is 100 degrees, the number of chirps is {lin.predict([ [ 100 ] ] )}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can put any number into the `lin.predict()` function and it will return what the model predicts the output should be. Want to put this predicted value on the graph?  Just add this before the `plt.show()` line (where it says #INSERT CODE HERE):\n",
    "\n",
    "    plt.plot(100, lin.predict([ [ 100 ] ]), 'g^')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell\n",
    "\n",
    "plt.figure() #resets the plot, just something you have to do in jupyter notebook if you also want it to be interactive\n",
    "\n",
    "plt.plot(features, label, 'ro') #plots the features\n",
    "\n",
    "#INSERT CODE HERE\n",
    "\n",
    "plt.show() #shows the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here is one annoying thing about `matplotlib`... it can't draw just a plain ol' simple line.  It'll connect points into a line, but can't draw a line given an equation.  It'd be nice to put the model's prediction on the same graph as the original data, so we have to work out a way to do this.\n",
    "\n",
    "To accomplish this I'm going to create a bunch of points that will make up our prediction line by using the input data and then plugging it into the equation for a line and getting the predicted y-value (our output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = (slope * features + inter) #this is an elementwise operation, which you can learn more about in my optional numpy unit\n",
    "\n",
    "plt.plot(features, y_vals, 'b-') #notice how this adds the line to the plot above! this is because I didn't call plot.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you put all this together, your graph should look like this (notice the predicted value for 100 degrees in the top-right corner of the graph):\n",
    "\n",
    "<img src=\"img/graph5.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING AND TESTING\n",
    "\n",
    "Remember that the whole point of all of this is to create a model, be able to take some new input data, plug it in to the model, and get a valid prediction.\n",
    "\n",
    "Of course, having more input data would help reduce the error (cost) of the model, but sometimes it's impossible, too difficult, or too costly (pun intended!) to get more input data.  Well, what about taking some of the original data and plugging it into the model to see what it predicts?  This isn't a good idea because that data was used to create the model, so it's going to always say that the model is good.\n",
    "\n",
    ">**opt:** Here's an interesting tidbit: machine learning can be very smart. If you train your neural network on all of your data, it might be able to just brute force the solutions. So instead of actually learning how to predict outputs based on inputs, it just memorizes the inputs and outputs you've given it. It's like in some of your other classes you might memorize a formula by doing one type of problem with it over and over again, but the problem might be phrased differently on a test and you would have no idea how to do it! More on this later...\n",
    "\n",
    "Is there anything else we could try?  Definitely.\n",
    "\n",
    "We can split up the original data into 2 sets:  **training data and testing data**.  Usually, we break up the original data like this:\n",
    "\n",
    "**75% of the original data = training dataset<br>\n",
    "25% of the original data = testing dataset**\n",
    "\n",
    "<img src=\"img/diagram1.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "Then, we use only the training data to create the model.  We \"train\" the model for what we're looking for.\n",
    "\n",
    "Once we have a model, we use the testing data to see how accurate the model is.  Remember, the testing data was never used to create the model.  So, it's like we found some new data to use.\n",
    "\n",
    "Also, in case you're curious, it's always better to randomly choose the training and testing data, not to just pick the first 75% of the original data and call it the training set.  Why?  Because the data may be sorted or skewed in some way, so randomly choosing the training and testing data keeps things honest.\n",
    "\n",
    "Luckily, and not surprisingly because this is python, there is a library function that will take care of all of this for us.  By default, it splits the data in training (75%) and testing (25%), and it randomly picks which data points go in each category every time it runs.  All we have to do is give the function our features list and label list, and it'll do the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, label_train, label_test = train_test_split(features, label)\n",
    "\n",
    "print(features_train.shape, features_test.shape, label_train.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our original dataset into training and test data, let's plot each on the graph and find the error score for each. \n",
    "\n",
    "So, replace some lines in your code with these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell\n",
    "\n",
    "plt.figure() #resets the plot\n",
    "\n",
    "plt.plot(features_train, label_train, 'ro') #plots the training labels with red dots\n",
    "plt.plot(features_test, label_test, 'k*') #plots the testing labels with black stars\n",
    "\n",
    "lin = LinearRegression().fit(features_train, label_train) #performs linear regression\n",
    "slope = lin.coef_ #sets the slope\n",
    "inter = lin.intercept_ #sets the intercept\n",
    "\n",
    "print (f'the training score is: {lin.score(features_train, label_train)}')\n",
    "print (f'the testing score is: {lin.score(features_test, label_test)}')\n",
    "\n",
    "y_vals = (slope * features_train + inter) #creates y values for the line of best fit\n",
    "\n",
    "plt.plot(features_train, y_vals, 'b-') #plots the line of best fit\n",
    "\n",
    "plt.show() #shows the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run this code, note which data points are in the training (red circle) and testing (black star) datasets.  Then, run the cell with the `train_test_split(...)` function and then the cell with the plotting code again and notice which data points change, how the slope and y-intercept change, and how the accuracy changes.  This is because the `train_test_split()` function is randomly selected different data points each time.\n",
    "\n",
    "(It might make it easier if you just copy and paste the code that splits the training and testing data into the top of the plotting cell, so you only have to run one cell)\n",
    "\n",
    "For our little cricket chirps dataset, splitting the data into training and testing isn't going to really matter.  The point of this is just to explain these concepts on a simple dataset, so you intuitively understand what is happening when we use the same techniques on much more complicated datasets.\n",
    "\n",
    "# MORE DATASETS\n",
    "\n",
    "Now that you've seen how to create a model with a dataset, how to plot the data, and how to test the accuracy of the model, it's time for you to try things on your own with larger datasets.\n",
    "\n",
    "Things to think about...\n",
    ">1) What does it mean when the training and testing scores are similar?<br>\n",
    ">2) What does it mean when the training score is much better than the testing score?<br>\n",
    ">3) What does it mean when the testing score is much better than the training score<br>\n",
    "\n",
    "# PRACTICE\n",
    "\n",
    "### Child Heights\n",
    "Try creating a linear model for either the boys or girls CDC height data.\n",
    "(the .CSV's are in the `data` folder in unit 2), so the pathstring would be `'/data/FILENAME.csv'`\n",
    "\n",
    "Predict what the height of the child will be at 0 years, 1 year, and 18 years.  Do these predictions make sense?\n",
    "\n",
    "Want some programming practice... the heights are in cm.  Try converting them to inches.  Or even feet and inches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert all the code you want here! refer to the code elswhere in the notebook if you need help\n",
    "#or create a new cell above or below this by using the insert tab in the toolbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Seattle Home Prices\n",
    "I've taken a dataset of Seattle home prices and created a separate CSV for a number of factors (home size, lot size, number of bedrooms, number of bathrooms, and year built). They're in the `seattle_housing_data` folder in the `data` folder, so the pathstring would be `'/data/seattle_housing_data/FILENAME.csv'`\n",
    "\n",
    "Pick one of the files and create a linear regression model.  Take note of which column has the input data and which column has the output data.\n",
    "\n",
    "Get the accuracy of your model. Then, compare it to others in the class who are working with other housing factors. From this can we say which home factor plays the largest role in home price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert all the code you want here!\n",
    "#or create a new cell above or below this by using the insert tab in the toolbar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
